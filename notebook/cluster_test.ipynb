{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 针对 聚类联邦学习 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from typing import List\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from fedlab.utils.serialization import SerializationTool\n",
    "from fedlab.utils.aggregator import Aggregators\n",
    "\n",
    "# 将项目根目录加入环境变量\n",
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.append(PROJECT_DIR)\n",
    "print(PROJECT_DIR)\n",
    "\n",
    "from utils import read_options\n",
    "from client_utils import detail_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, cluster_partitioner, model = read_options()\n",
    "random.seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_client = config['num_client']\n",
    "num_classes = config['num_classes']\n",
    "\n",
    "num_round = config['num_round']\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_sample_stream = [\n",
    "    random.sample(\n",
    "        range(num_client), max(1, int(num_client * 0.2))\n",
    "    )\n",
    "    for _ in range(num_round)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    client_models: List[torch.nn.Module] = [model() for _ in range(num_client)]\n",
    "    client_optimizers: List[torch.optim.SGD] = [torch.optim.SGD(client_models[i].parameters(), lr=config['lr']) for i in range(num_client)]\n",
    "    client_criteria: List[torch.nn.CrossEntropyLoss] = [torch.nn.CrossEntropyLoss() for _ in range(num_client)]\n",
    "\n",
    "    global_model: torch.nn.Module = model()\n",
    "    global_optimizer: torch.optim.SGD = torch.optim.SGD(global_model.parameters(), lr=config['lr'])\n",
    "    global_criterion: torch.nn.CrossEntropyLoss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    return client_models, client_optimizers, client_criteria, global_model, global_optimizer, global_criterion\n",
    "\n",
    "def train(cid: int, model: torch.nn.Module, optimizer: torch.optim.SGD, criterion: torch.nn.CrossEntropyLoss):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loader = cluster_partitioner.get_dataloader(cid, config['local_bs'])\n",
    "    for _ in range(config['local_ep']):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints\n",
    "\n",
    "CFL 算法过程测试\n",
    "\n",
    "结果：判断是否二分的条件中存在人工给定的两个超参数, 目前使用原论文所给代码的原始超参数的情况下, 始终无法达到对应的 if 条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CFL_test():\n",
    "    client_models, client_optimizers, client_criteria, global_model, _, _ = model_init()\n",
    "    client_cluster = [list(range(num_client))]\n",
    "\n",
    "    for current_round in range(num_round):\n",
    "        \n",
    "        if current_round == 0:\n",
    "            for cid in range(num_client):\n",
    "                client_models[cid].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        grads_list: List[torch.Tensor] = [None] * num_client\n",
    "        grads_list = np.array(grads_list)\n",
    "        selected_clients: List[int] = client_sample_stream[current_round]\n",
    "        for client_id in selected_clients:\n",
    "            \n",
    "            local_model: torch.nn.Module = client_models[client_id]\n",
    "            local_optimizer = client_optimizers[client_id]\n",
    "            local_criterion = client_criteria[client_id]\n",
    "            \n",
    "            param_before = SerializationTool.serialize_model(local_model).detach()\n",
    "            train(client_id, local_model, local_optimizer, local_criterion)\n",
    "            param_after = SerializationTool.serialize_model(local_model).detach()\n",
    "            \n",
    "            grads = param_before - param_after\n",
    "            grads_list[client_id] = grads\n",
    "        \n",
    "        similarity_matrix = torch.zeros((num_client, num_client))\n",
    "        for i, grads_i in enumerate(grads_list):\n",
    "            for j, grads_j in enumerate(grads_list[i + 1:], i + 1):\n",
    "                if grads_i is not None and grads_j is not None:\n",
    "                    similarity_score = torch.cosine_similarity(\n",
    "                        grads_i, grads_j, dim=0, eps=1e-12\n",
    "                    ).item()\n",
    "                    similarity_matrix[i, j] = similarity_score\n",
    "                    similarity_matrix[j, i] = similarity_score\n",
    "        similarity_matrix = similarity_matrix.numpy()\n",
    "        print(\"similarity_matrix: \\n{}\".format(similarity_matrix[selected_clients][:, selected_clients]))\n",
    "        \n",
    "        client_cluster_new = []\n",
    "        for indices in client_cluster:\n",
    "            max_norm = np.max([torch.norm(grads_list[i]).item() for i in indices if grads_list[i] is not None])\n",
    "            mean_norm = torch.norm(torch.mean(torch.stack([grads_list[i] for i in indices if grads_list[i] is not None]), dim=0)).item()\n",
    "            print(\"max_norm: {}, mean_norm: {}, len(cluster): {}, current round: {}\".format(round(max_norm, 4), round(mean_norm, 4), len(indices), current_round))\n",
    "            if mean_norm < 0.4 and max_norm > 1.6 and len(indices) > 2 and current_round > 20:\n",
    "                clustering = AgglomerativeClustering(\n",
    "                    affinity='precomputed', linkage='complete'\n",
    "                ).fit(-similarity_matrix[indices][:, indices])\n",
    "                cluster_1 = np.argwhere(clustering.labels_ == 0).flatten()\n",
    "                cluster_2 = np.argwhere(clustering.labels_ == 1).flatten()\n",
    "                print(\"cluster {} is split into {} and {}\".format(indices, cluster_1, cluster_2))\n",
    "                client_cluster_new += [cluster_1, cluster_2]\n",
    "            else:\n",
    "                client_cluster_new += [indices]\n",
    "\n",
    "        client_cluster = client_cluster_new\n",
    "        \n",
    "        for cluster in client_cluster:\n",
    "            average_grads = Aggregators.fedavg_aggregate([grads_list[cid] for cid in cluster if grads_list[cid] is not None])\n",
    "            for cid in cluster:\n",
    "                client_models[cid].to('cpu')\n",
    "                SerializationTool.deserialize_model(client_models[cid], average_grads, mode='add')\n",
    "        print(\"round {} is finished\".format(current_round))\n",
    "\n",
    "CFL_test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Greedy Agglomerative Framework for Clustered Federated Learning\n",
    "\n",
    "Federated Learning via Agglomerative Client Clustering (FLACC) 算法\n",
    "\n",
    "效果：有一定的聚类效果, 但是效果依旧不明显\n",
    "\n",
    "思考：能不能将聚合的判断条件类比到基于密度的聚合原理中, 利用其中所谓的临近节点等概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLACC_test():\n",
    "    client_models, client_optimizers, client_criteria, global_model, _, _ = model_init()\n",
    "    similarity_matrix = np.full((num_client, num_client), -1.0, dtype=np.float32)\n",
    "    memory_matrix = np.full((num_client, num_client), 0, dtype=np.int32)    \n",
    "    client_cluster, model_cluster = [[i] for i in range(num_client)], [model() for i in range(num_client)]\n",
    "    client_cluster_model = {i: model_cluster[i] for i in range(num_client)}\n",
    "    \n",
    "    no_merge_count = 0\n",
    "    \n",
    "    for current_round in range(num_round):\n",
    "        grads_list: List[torch.Tensor] = [None] * num_client\n",
    "        selected_clients = client_sample_stream[current_round]\n",
    "        print(\"selected clients: {}\".format(sorted(selected_clients)))\n",
    "        if no_merge_count < 10:\n",
    "            for cid in selected_clients:\n",
    "                local_model: torch.nn.Module = client_models[cid]\n",
    "                local_model.load_state_dict(global_model.state_dict())\n",
    "                local_optimizer = client_optimizers[cid]\n",
    "                local_criterion = client_criteria[cid]\n",
    "                \n",
    "                param_before = SerializationTool.serialize_model(local_model).detach()\n",
    "                train(cid, local_model, local_optimizer, local_criterion)\n",
    "                param_after = SerializationTool.serialize_model(local_model).detach()\n",
    "                \n",
    "                grads = param_before - param_after\n",
    "                grads_list[cid] = grads\n",
    "            \n",
    "            for i, cid_i in enumerate(selected_clients):\n",
    "                for j, cid_j in enumerate(selected_clients[i+1:], i+1):\n",
    "                    if grads_list[cid_i] is not None and grads_list[cid_j] is not None:\n",
    "                        similarity_score = torch.cosine_similarity(\n",
    "                            grads_list[cid_i], grads_list[cid_j], dim=0, eps=1e-12\n",
    "                        ).item()\n",
    "                        similarity_matrix[cid_i, cid_j] = similarity_score\n",
    "                        similarity_matrix[cid_j, cid_i] = similarity_score\n",
    "                        \n",
    "                        memory_matrix[cid_i, cid_j] = current_round\n",
    "                        memory_matrix[cid_j, cid_i] = current_round\n",
    "            \n",
    "            max_min_similarity = float('-inf')\n",
    "            cluster1_index, cluster2_index = -1, -1\n",
    "            cross_max_similarity = None\n",
    "            within_min_similarity = None\n",
    "            cross_min_similarity = None\n",
    "            for i, cluster_i in enumerate(client_cluster):\n",
    "                for j, cluster_j in enumerate(client_cluster[i + 1: ], i + 1):\n",
    "                    \n",
    "                    if len(cluster_i) == 1 and len(cluster_j) == 1:\n",
    "                        min_similarity = similarity_matrix[cluster_i[0], cluster_j[0]]\n",
    "                    else:\n",
    "                        mask = similarity_matrix[cluster_i][:, cluster_j] > -1.0\n",
    "                        if np.any(mask):\n",
    "                            min_similarity = np.min(similarity_matrix[cluster_i][:, cluster_j][mask])\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    if min_similarity > max_min_similarity:\n",
    "                        \n",
    "                        if len(cluster_i) > 1 and len(cluster_j) > 1:\n",
    "                            mask1 = similarity_matrix[cluster_i][:, cluster_i] > -1.0\n",
    "                            mask2 = similarity_matrix[cluster_j][:, cluster_j] > -1.0\n",
    "                            min_cluster_i = np.min(similarity_matrix[cluster_i][:, cluster_i][mask1]) if np.any(mask1) else float('inf')\n",
    "                            min_cluster_j = np.min(similarity_matrix[cluster_j][:, cluster_j][mask2]) if np.any(mask2) else float('inf')\n",
    "                            if math.isinf(min_cluster_i) and math.isinf(min_cluster_j):\n",
    "                                continue\n",
    "                            within_min_similarity = min(min_cluster_i, min_cluster_j)\n",
    "\n",
    "                        max_min_similarity = min_similarity\n",
    "                        cluster1_index, cluster2_index = i, j\n",
    "                        cross_max_similarity =  np.max(similarity_matrix[cluster_i][:, cluster_j])\n",
    "                        cross_min_similarity =  min_similarity\n",
    "            \n",
    "            print(\"cross_min_similarity: {}, cross_max_similarity: {}, within_min_similarity: {}\".format(\n",
    "                round(float(cross_min_similarity), 4), round(float(cross_max_similarity), 4), within_min_similarity\n",
    "            ))\n",
    "            if cross_min_similarity > 0 and (within_min_similarity is None or cross_max_similarity > within_min_similarity):\n",
    "                print(\"merge {} and {}\".format(client_cluster[cluster1_index], client_cluster[cluster2_index]))\n",
    "                cluster1 = client_cluster[cluster1_index]\n",
    "                cluster2 = client_cluster[cluster2_index]\n",
    "                client_cluster.remove(cluster1)\n",
    "                client_cluster.remove(cluster2)\n",
    "                client_cluster.append(cluster1 + cluster2)\n",
    "                \n",
    "                model1 = model_cluster[cluster1_index]\n",
    "                model2 = model_cluster[cluster2_index]\n",
    "                model_cluster.remove(model1)\n",
    "                model_cluster.remove(model2)\n",
    "                model_param_lsit = [SerializationTool.serialize_model(client_models[cid]) for cid in cluster1 + cluster2]\n",
    "                avg_param = Aggregators.fedavg_aggregate(model_param_lsit)\n",
    "                cluster_model = model()\n",
    "                SerializationTool.deserialize_model(cluster_model, avg_param)\n",
    "                model_cluster.append(cluster_model)\n",
    "                \n",
    "                for cid in cluster1 + cluster2:\n",
    "                    client_cluster_model[cid] = cluster_model\n",
    "\n",
    "                no_merge_count = 0\n",
    "            \n",
    "            else:\n",
    "                no_merge_count += 1\n",
    "            \n",
    "            out_memory_indices = np.where((current_round - memory_matrix) > 10)\n",
    "            similarity_matrix[out_memory_indices] = -1\n",
    "            \n",
    "            selected_clients_param_list = [SerializationTool.serialize_model(client_models[cid]) for cid in selected_clients]\n",
    "            selected_clients_avg_param = Aggregators.fedavg_aggregate(selected_clients_param_list)\n",
    "            SerializationTool.deserialize_model(global_model, selected_clients_avg_param)\n",
    "        \n",
    "        else:\n",
    "            for cid in selected_clients:\n",
    "                local_model: torch.nn.Module = client_models[cid]\n",
    "                local_model.load_state_dict(client_cluster_model[cid].state_dict())\n",
    "                local_optimizer = client_optimizers[cid]\n",
    "                local_criterion = client_criteria[cid]\n",
    "                \n",
    "                train(cid, local_model, local_optimizer, local_criterion)\n",
    "            \n",
    "            selected_clients_set = set(selected_clients)\n",
    "            for index, cluster in enumerate(client_cluster):\n",
    "                cluster_set = set(cluster)\n",
    "                intersection = selected_clients_set.intersection(cluster_set)\n",
    "                if len(intersection) > 0:\n",
    "                    print(\"update cluster {} using clients {}\".format(cluster, intersection))\n",
    "                    param_list = [SerializationTool.serialize_model(client_models[cid]) for cid in intersection]\n",
    "                    avg_param = Aggregators.fedavg_aggregate(param_list)\n",
    "                    SerializationTool.deserialize_model(model_cluster[index], avg_param)\n",
    "\n",
    "    # test\n",
    "    global_eval = np.zeros(num_classes + 1)\n",
    "    for cluster_cids, cluster_model in zip(client_cluster, model_cluster):\n",
    "        test_loader = cluster_partitioner.get_cluster_dataloader(cluster_cids, config['local_bs'], \"test\")\n",
    "        result = np.array(detail_evaluate(cluster_model, torch.nn.CrossEntropyLoss(), test_loader, num_classes))\n",
    "        global_eval += result\n",
    "    global_eval /= len(client_cluster)\n",
    "    global_eval = np.round(global_eval, 4)\n",
    "    print(\"global_eval: {}\".format(global_eval.tolist()))\n",
    "        \n",
    "\n",
    "FLACC_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于 FLACC 的改进\n",
    "使用二分进一步验证划分的可靠性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clusters(client_cluster, model_cluster):\n",
    "    print(\"evaluate_clusters:\")\n",
    "    num_classes = config['num_classes']\n",
    "    global_eval = np.zeros(num_classes + 1)\n",
    "    for cluster_cids, cluster_model in zip(client_cluster, model_cluster):\n",
    "        print(\"cluster_cids: {}\".format(cluster_cids))\n",
    "        test_loader = cluster_partitioner.get_cluster_dataloader(cluster_cids, config['local_bs'], \"test\")\n",
    "        result = np.array(detail_evaluate(cluster_model, torch.nn.CrossEntropyLoss(), test_loader, num_classes))\n",
    "        global_eval += result\n",
    "    global_eval /= len(client_cluster)\n",
    "    global_eval = np.round(global_eval, 4)\n",
    "    print(\"global_eval: {}\".format(global_eval.tolist()))\n",
    "    return global_eval\n",
    "\n",
    "def create_cluster_model(cluster: List[int], client_models: List[torch.nn.Module]):\n",
    "    if len(cluster) == 1:\n",
    "        return client_models[cluster[0]]\n",
    "    model_param_lsit = [SerializationTool.serialize_model(client_models[cid]) for cid in cluster]\n",
    "    avg_param = Aggregators.fedavg_aggregate(model_param_lsit)\n",
    "    cluster_model = model()\n",
    "    SerializationTool.deserialize_model(cluster_model, avg_param)\n",
    "    return cluster_model\n",
    "\n",
    "def remove_clients_from_cluster(client_cluster, model_cluster, similarity_matrix, client_models):\n",
    "    client_cluster_new = []\n",
    "    model_cluster_new = []\n",
    "    for index, cluster in enumerate(client_cluster):\n",
    "        if len(cluster) < 2:\n",
    "            client_cluster_new.append(cluster)\n",
    "            model_cluster_new.append(model_cluster[index])\n",
    "            continue\n",
    "        cluster_similarity_matrix = similarity_matrix[cluster, :][:, cluster]\n",
    "        mask = np.where((cluster_similarity_matrix > -1.0) & (cluster_similarity_matrix < 0.0))\n",
    "        if not np.any(mask):\n",
    "            client_cluster_new.append(cluster)\n",
    "            model_cluster_new.append(model_cluster[index])\n",
    "            continue\n",
    "        outlier_indices = np.unique(mask)\n",
    "        print(\"remove {} from {}\".format(outlier_clients, cluster))\n",
    "\n",
    "        outlier_clients = []\n",
    "        reminder_clients = []\n",
    "        for cid in cluster:\n",
    "            if cid in outlier_indices:\n",
    "                outlier_clients.append(cid)\n",
    "            else:\n",
    "                reminder_clients.append(cid)\n",
    "        if len(reminder_clients) > 0:\n",
    "            client_cluster_new.append(reminder_clients)\n",
    "            model_cluster_new.append(create_cluster_model(reminder_clients, client_models))\n",
    "        for cid in outlier_clients:\n",
    "            client_cluster_new.append([cid])\n",
    "            model_cluster_new.append(client_models[cid])\n",
    "        \n",
    "\n",
    "        # min_similarity = np.min(cluster_similarity_matrix[mask])\n",
    "        # if min_similarity < 0:\n",
    "        #     clustering = AgglomerativeClustering(\n",
    "        #         affinity=\"precomputed\", linkage=\"complete\"\n",
    "        #     ).fit(-cluster_similarity_matrix)\n",
    "        #     cluster_1 = [cluster[i] for i in np.argwhere(clustering.labels_ == 0).flatten()]\n",
    "        #     cluster_2 = [cluster[i] for i in np.argwhere(clustering.labels_ == 1).flatten()]\n",
    "        #     if len(cluster_1) == 0 or len(cluster_2) == 0:\n",
    "        #         client_cluster_new.append(cluster)\n",
    "        #         model_cluster_new.append(model_cluster[index])\n",
    "        #     else:\n",
    "        #         print(\"split {} into {} and {}\".format(cluster, cluster_1, cluster_2))\n",
    "        #         model_1 = create_cluster_model(cluster_1, client_models)\n",
    "        #         model_2 = create_cluster_model(cluster_2, client_models)\n",
    "        #         client_cluster_new += [cluster_1, cluster_2]\n",
    "        #         model_cluster_new += [model_1, model_2]\n",
    "        # else:\n",
    "        #     client_cluster_new.append(cluster)\n",
    "        #     model_cluster_new.append(model_cluster[index])\n",
    "    return client_cluster_new, model_cluster_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_test():\n",
    "    client_models, client_optimizers, client_criteria, global_model, _, _ = model_init()\n",
    "    similarity_matrix = np.full((num_client, num_client), -1.0, dtype=np.float32)\n",
    "    memory_matrix = np.full((num_client, num_client), 0, dtype=np.int32)\n",
    "    client_cluster = [[i] for i in range(num_client)]\n",
    "    model_cluster = [model() for i in range(num_client)]\n",
    "    client_cluster_model = {i: model_cluster[i] for i in range(num_client)}\n",
    "\n",
    "    no_merge_count = 0\n",
    "\n",
    "    for current_round in range(num_round):\n",
    "        grads_list: List[torch.Tensor] = [None] * num_client\n",
    "        selected_clients = client_sample_stream[current_round]\n",
    "        if no_merge_count < 10:\n",
    "            for cid in selected_clients:\n",
    "                local_model: torch.nn.Module = client_models[cid]\n",
    "                local_model.load_state_dict(global_model.state_dict())\n",
    "                local_optimizer = client_optimizers[cid]\n",
    "                local_criterion = client_criteria[cid]\n",
    "                \n",
    "                param_before = SerializationTool.serialize_model(local_model).detach()\n",
    "                train(cid, local_model, local_optimizer, local_criterion)\n",
    "                param_after = SerializationTool.serialize_model(local_model).detach()\n",
    "                \n",
    "                grads = param_before - param_after\n",
    "                grads_list[cid] = grads\n",
    "            \n",
    "            for i, cid_i in enumerate(selected_clients):\n",
    "                for j, cid_j in enumerate(selected_clients[i+1:], i+1):\n",
    "                    if grads_list[cid_i] is not None and grads_list[cid_j] is not None:\n",
    "                        similarity_score = torch.cosine_similarity(\n",
    "                            grads_list[cid_i], grads_list[cid_j], dim=0, eps=1e-12\n",
    "                        ).item()\n",
    "                        similarity_matrix[cid_i, cid_j] = similarity_score\n",
    "                        similarity_matrix[cid_j, cid_i] = similarity_score\n",
    "\n",
    "                        memory_matrix[cid_i, cid_j] = current_round\n",
    "                        memory_matrix[cid_j, cid_i] = current_round\n",
    "                        \n",
    "            # 从 cluster 中删除 client (二分)\n",
    "            client_cluster, model_cluster = remove_clients_from_cluster(\n",
    "                client_cluster, model_cluster, similarity_matrix, client_models\n",
    "            )\n",
    "            \n",
    "            # 合并 cluster\n",
    "            max_min_similarity = float(\"-inf\")\n",
    "            cluster1_index, cluster2_index = -1, -1\n",
    "            cross_max_similarity = None\n",
    "            within_min_similarity = None\n",
    "            cross_min_similarity = None\n",
    "            for i, cluster_i in enumerate(client_cluster):\n",
    "                for j, cluster_j in enumerate(client_cluster[i+1:], i+1):\n",
    "                    \n",
    "                    if len(cluster_i) == 1 and len(cluster_j) == 1:\n",
    "                        min_similarity = similarity_matrix[cluster_i[0], cluster_j[0]]\n",
    "                    else:\n",
    "                        mask = similarity_matrix[cluster_i][:, cluster_j] > -1.0\n",
    "                        if not np.any(mask):\n",
    "                            continue\n",
    "                        min_similarity = np.min(similarity_matrix[cluster_i][:, cluster_j][mask])\n",
    "                    \n",
    "                    if min_similarity <= max_min_similarity:\n",
    "                        continue\n",
    "\n",
    "                    if len(cluster_i) > 1 and len(cluster_j) > 1:\n",
    "                        mask1 = similarity_matrix[cluster_i][:, cluster_i] > -1.0\n",
    "                        mask2 = similarity_matrix[cluster_j][:, cluster_j] > -1.0\n",
    "                        min_cluster_i = np.min(similarity_matrix[cluster_i][:, cluster_i][mask1]) if np.any(mask1) else float('inf')\n",
    "                        min_cluster_j = np.min(similarity_matrix[cluster_j][:, cluster_j][mask2]) if np.any(mask2) else float('inf')\n",
    "                        if math.isinf(min_cluster_i) and math.isinf(min_cluster_j):\n",
    "                            continue\n",
    "                        within_min_similarity = min(min_cluster_i, min_cluster_j)\n",
    "\n",
    "                    max_min_similarity = min_similarity\n",
    "                    cluster1_index, cluster2_index = i, j\n",
    "                    cross_max_similarity =  np.max(similarity_matrix[cluster_i][:, cluster_j])\n",
    "                    cross_min_similarity =  min_similarity\n",
    "                \n",
    "            print(\"cross_min_similarity: {}, cross_max_similarity: {}, within_min_similarity: {}\".format(\n",
    "                round(float(cross_min_similarity), 4), round(float(cross_max_similarity), 4), within_min_similarity\n",
    "            ))\n",
    "            if cross_min_similarity > 0 and (within_min_similarity is None or cross_max_similarity > within_min_similarity):\n",
    "                print(\"merge {} and {}\".format(client_cluster[cluster1_index], client_cluster[cluster2_index]))\n",
    "                cluster1 = client_cluster[cluster1_index]\n",
    "                cluster2 = client_cluster[cluster2_index]\n",
    "                client_cluster.remove(cluster1)\n",
    "                client_cluster.remove(cluster2)\n",
    "                client_cluster.append(cluster1 + cluster2)\n",
    "                \n",
    "                model1 = model_cluster[cluster1_index]\n",
    "                model2 = model_cluster[cluster2_index]\n",
    "                model_cluster.remove(model1)\n",
    "                model_cluster.remove(model2)\n",
    "                cluster_model = create_cluster_model(cluster1 + cluster2, client_models)\n",
    "                model_cluster.append(cluster_model)\n",
    "                \n",
    "                for cid in cluster1 + cluster2:\n",
    "                    client_cluster_model[cid] = cluster_model\n",
    "\n",
    "                no_merge_count = 0\n",
    "            \n",
    "            else:\n",
    "                no_merge_count += 1\n",
    "\n",
    "            out_memory_indices = np.where((current_round - memory_matrix) > 10)\n",
    "            similarity_matrix[out_memory_indices] = -1\n",
    "            \n",
    "            selected_clients_param_list = [SerializationTool.serialize_model(client_models[cid]) for cid in selected_clients]\n",
    "            selected_clients_avg_param = Aggregators.fedavg_aggregate(selected_clients_param_list)\n",
    "            SerializationTool.deserialize_model(global_model, selected_clients_avg_param)\n",
    "        \n",
    "        else:\n",
    "            for cid in selected_clients:\n",
    "                local_model: torch.nn.Module = client_models[cid]\n",
    "                local_model.load_state_dict(client_cluster_model[cid].state_dict())\n",
    "                local_optimizer = client_optimizers[cid]\n",
    "                local_criterion = client_criteria[cid]\n",
    "                \n",
    "                train(cid, local_model, local_optimizer, local_criterion)\n",
    "            \n",
    "            selected_clients_set = set(selected_clients)\n",
    "            for index, cluster in enumerate(client_cluster):\n",
    "                cluster_set = set(cluster)\n",
    "                intersection = selected_clients_set.intersection(cluster_set)\n",
    "                if len(intersection) > 0:\n",
    "                    print(\"update cluster {} using clients {}\".format(cluster, intersection))\n",
    "                    param_list = [SerializationTool.serialize_model(client_models[cid]) for cid in intersection]\n",
    "                    avg_param = Aggregators.fedavg_aggregate(param_list)\n",
    "                    SerializationTool.deserialize_model(model_cluster[index], avg_param)\n",
    "\n",
    "improve_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
